\section{Discussion}
\label{sec:discussion}

Here, we discuss possible reasons for the results presented
 in section \ref{sec:results}.
As the Web is a socio-technical construct,
 we focus not only on technical reasons for lack of machine processability
 but also on social reasons.

Technically, we see many of the common errors identified by
 the Pedantic Web group\footnote{See~\cite{conf/www/HoganHPDP10} and
   Frequently Observed Problems on the Web of Data: \url{http://pedantic-web.org/fops.html}}.
Many of these come from difficulties in configuring Web Servers
 or appropriately applying standards for access.
For example, setting up content-negotiation is often difficult;
 even making sure content-types are properly defined.
We also believe that many simple syntax errors stem from
 the use of common UNIX text manipulation tools
 (e.g. \texttt{grep}, \texttt{sed}).
This is evidenced by our own experience talking to developers
 as well as investigating some of the datasets by hand.
Also, the mismatch between actual content and file extension
 widely occurs because of multiple flavors of RDF, in particular,
 many of the text formats overlap (e.g. is it a Trig or Turtle serialization?).

It is important to note that issues with providing quality data
 are not necessarily a function of Linked Data formats (i.e. RDF).
Most data scientists can easily attest to the difficulties
 in processing data \cite{baddatabook}.
These vary from data being designed for human consumption\footnote{e.g.
  Laid out in a spreadsheet to be visually appealing
  and not to be processed.}
 to being encoded in a difficult to parse format.\footnote{e.g.
  A CSV file provided as a PDF.}
The site \url{http://okfnlabs.org/bad-data/} provides several examples
 of machine unfriendly data.

Much of these technical issues have to do with lack of tooling
 for both data consumption as well as publication.
For instance the Billion Triple Challenge\footnote{\url{http://km.aifb.kit.edu/projects/btc-2012/}}
 contains many files that cannot be loaded by any standards-compliant tool.
Many of these bigger data dumps seem to have only been processed
 by text tools, i.e. without fully parsing the syntax of
 the data serialization language.
At the same time, we are still far from the current state that we have
 on the Web of Documents, where we have sophisticated programs
 that are tolerant of errors in data (e.g. Web Crawlers and Browsers).
Nor do we have a similar rich environment of guidelines
 and tools for publication
 (e.g. Webmaster guidelines\footnote{\url{https://support.google.com/webmasters/answer/35769?hl=en}},
  content publishing platforms\footnote{\url{http://wordpress.org}}).

Importantly, these technical issues cannot be separated from
 the social environment that data publication resides in.
Data publication -- and in particular Linked Data publication --
 has numerous social issues that impact both its quality
 and availability for use by machines.
For example, those datasets with established communities
 have a better chance for staying up:
 biological and chemical datasets in their Linked Data form
 are generally of better quality.
They are often backed by organizations who have as a purpose
 making such resources available, for example,
 the European Bioinformatics Institute,
 the National Center for Bio-ontology,
 or the Swiss Institute for Bioinformatics.
They also have the requisite expertise and financing for
 maintaining this data\footnote{See for example,
   the EBI RDF platform \url{http://www.ebi.ac.uk/rdf/about-rdf-platform}}.
An interesting hallmark of Linked Data is that it is often third-parties
 that provide the Linked Data version of a dataset \cite{Heath2011}.
Thus, the incentive structure for providing data
 (i.e access to an RDF version) may not be aligned with
 long term availability of the data.
As others have noted~\cite{bismodelslogd},
 it is critical to have a business model behind opening data.
Moving towards machine friendly publication will require
 a clear business case for data providers.


