\section{Approach}
\label{sec:approach}

In contrast to the explicit schema information that should be present
in an RDF data set in the form of \texttt{rdf:type} specifications, in
this section we will use the structural properties of the graph to
extract implicit schema information.
structural properties of the graph can be used

In the previous section we saw that both excluding and including object terms
 in the partitioning of resources has negative effects on some data.
In this section we seek the best of both worlds
 and generalize these approaches using a partition $\M$ of resources.

Let $\tn{class}(t) = C$ iff $t \in C$ and $C \in \M$,
 thus the class of a term is its partition cell.
We now define the generic fingerprint (called $f_\M$) of a subject term $s$
 in Definition \ref{def:fingerprint}.

\begin{definition}[Fingerprint]
  \label{def:fingerprint}
  $
    f_\M(s)
  := \\
    \{
      \tuple{p,C} \in P_G \times \mathcal{P}(O_G)
    \mid
      \pair{I(s)}{I(o)} \in Ext(I(p)) \, \wedge \, o \in C \\
      \wedge \, C \in \M
    \}
  $
\end{definition}

The fingerprints $f_2$ and $f_3$ in Section \ref{sec:fingerprints}
 are instantiations of the generic fingerprint $f_\M$.
They correspond to, respectively,
  a partition $\M$ that contains a single set of all terms,
 and
  a partition $\M$ that contains all terms as singleton sets.

The advantage of generalizing these methods using $f_m$ is that
 a well chosen $\M$ can interpolate between these two existing approaches,
 and retain all relevant information about the term while discarding
 irrelevant details.
Our method of selecting a suitable partition is
 described in Section~\ref{sec:mdl} below.

\subsection{MDL Model Selection}
\label{sec:mdl}

A well motivated approach to model selection is based on
 the Minimum Description Length (MDL) principle
 \cite{Rissanen78,Rissanen84,grunwald2007}.
MDL can be interpreted as a formalization of Occam's Razor,
 which states that a more simple hypothesis,
 or model, should be preferred over a more complicated explanation of
 the same data.
In MDL, the words ``simple'' and ``complicated'' are
 made precise by using codes.
Given a space $\M$ of models, and data $D\in\D$,
 we have to specify:

\begin{itemize}
\item A code $C:\M\to\{0,1\}^*$ to encode the model
\item For each model $M\in\M$, a code $C_M:\D\to\{0,1\}^*$ to encode
  the data, making use of the model.
\end{itemize}

The second code uses the model in order to achieve an efficient encoding.
The best model is then the one that minimizes the overall code-length
 (equation \ref{eq:mdl}).

\begin{equation}
  \label{eq:mdl}
  M_\tn{mdl}:=\arg\min_{M\in\M}|C(M)| + |C_M(D)|.
\end{equation}

By balancing these two contributions
 (the coder and the data encoded using that codes)
 to the code length,
 MDL avoids very simple models that provide poor fit to the data
 (as these would have a long $C_M(D)$),
 and it also avoids overfitting,
 as overly complex models have a long $C(M)$.
Also note that the model selection procedure only relies on
 the code \emph{lengths}, so it suffices to define code length functions
 without worrying about the actual code words.
In the following we will write $L(x)$ as a shorthand for $|C(x)|$.

MDL model selection is very closely related to Bayes factors model selection,
 where the prior distribution on models replaces $C(M)$
and the likelihood function replaces $C_M(D)$;
readers more familiar with Bayesian methodology may wish to
 mentally make these substitutions and reading
 ``negative loglikelihood'' wherever we write ``code length''.

In our application,
 the model $\M$ is the partition that determines the fingerprints.
The construction of the code is outlined in the next section.



