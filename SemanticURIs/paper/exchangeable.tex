\documentclass{article}
\usepackage{amsmath,amsfonts,algorithms}

\author{Steven de Rooij and Wouter Beek}
\title{Compressing RDF data}

\newcommand{\B}{{\mathbb B}}
\newcommand{\X}{{\mathcal X}}
\newcommand{\U}{{\mathcal U}}
\newcommand{\BN}{{\mathcal B}}
\renewcommand{\L}{{\mathcal L}}
\newcommand{\D}{{\mathcal D}}
\renewcommand{\P}{{\mathcal P}}
\newcommand{\I}{{\mathcal I}}
\newcommand{\df}{:=}
\newcommand{\tn}{\textnormal}
\newcommand{\half}{\tfrac{1}{2}}
\newcommand{\ind}{\mathop{\mathbf{1}}}

\newcommand{\Psm}[1]{\tn{P}_{\tn{sm}(#1)}}
\newcommand{\Pkt}[1]{\tn{P}_{\tn{kt}(#1)}}
\renewcommand{\Pr}{\tn{P}_\tn{r}}

\bibliographystyle{plain}

\begin{document}
\maketitle


\section{Exchangable sparsity}

Consider a sequence $x^n=x_1,\ldots,x_n$ with symbols from a large,
but countable alphabet $\X$. Define $\#z(x^i)$ as the number of
occurrences of $z$ in $x^i$, abbreviating $\#z\df\#z(x^n)$.  Let 
$\U_i\df\{x_1,\ldots,x_i\}\subseteq\X$ 
be the symbols that actually occur in the sequence, and let $k_i\df|\U_i|$. abbreviate
$\U\df\U_n$ and $k\df k_n$. We have $\sum_{u\in\U}\#u=n$.

We want to encode each symbol only when it appears for the first time
in the sequence; for all subsequent occurrences we can get away with
encoding a reference instead. Our prediction strategy maintains the
frequencies of all past symbols, $\#u(x^i)$ for all $u\in\U_i$, and
the number of distinct symbols $k_i$. The strategy is similar to the
KT estimator. Every symbol has a counter, and every iteration the
counter for the symbol that occurs is incremented by one. The
normalization is the sum of the counters and the fake outcomes, and is
also always incremented by one. (Defining the prediction strategy in
such a way appears to be key in making it exchangeable.) The
difference is that with KT, the alphabet is known and fixed from the
start, whereas to predict the $i$th outcome, we will use
$\U_{i-1}\cup\{\textsf{n}\}$ as the alphabet, which grows with
$i$. The $\textsf{n}$ denotes the event that a new the symbol is new;
such a first occurrence is then transmitted explicitly using the
original model $P_0$. The new symbol receives count $1-\alpha$, which
is a parameter of the strategy, $\alpha\in(0,1)$; the \textsf{n}
symbol receives count $\alpha$ so in total the counts are incremented
by one. The $\textsf{n}$ symbol has an initial count of
$\beta\in(0,\infty)$, which is another parameter.

Let $P_0$ be the (exchangable) default model for outcomes from
$\X$. $P_0$ works for our data but is not efficient when there are many
repetitions in the data. Then the prediction strategy is defined by
\[
P(x_i\mid x^i)\df\begin{cases}
\displaystyle\frac{\#x_i(x^{i-1})-\alpha}{i-1+\beta}&\tn{if $x_i\in\U_{i-1}$,}\\
\displaystyle\frac{\alpha k_{i-1}+\beta}{i-1+\beta}\cdot
P_0(x_i\mid x_i\ne\U_{i-1})&\tn{if $x_i$ is new}.
\end{cases}
\]
Note that the first symbol is always new, and
$P(x_1)=P(\textsf{n})P_0(x_1)=(\beta/\beta)P_0(x_1)$.

Multiplying this out we obtain
\[\begin{split}
\frac{P(x^n)}{P_0(\tn{unique subsequence})}&=\frac{\overbrace{\left(\prod_{u\in\U}
    \frac{\Gamma(\#u-\alpha)}{\Gamma(1-\alpha)}\right)}^{\tn{repeating
  symbols}}\cdot\overbrace{\alpha^k\frac{\Gamma(k+\frac{\beta}{\alpha})}{\Gamma(\frac{\beta}{\alpha})}}^{\tn{new
  symbols}}}{\displaystyle\frac{\Gamma(n+\beta)}{\Gamma(\beta)}}\\
&=\frac{\Gamma(\beta)\left(\prod_{u\in\U}
   \Gamma(\#u-\alpha)\right)\cdot \alpha^k\Gamma(k+\frac{\beta}{\alpha})}{\Gamma(n+\beta)\Gamma(1-\alpha)^k\Gamma(\frac{\beta}{\alpha})}.
\end{split}
\]
To find dependence on $n$, we using Stirling's approximation to rewrite
%
\[\begin{split}
-\ln\frac{P}{P_0} =& 
\sum_u\left(
-(\#u-\alpha-\tfrac{1}{2})\ln(\#u-\alpha-1)+(\#u-\alpha-1)\right)\\
&+(n+\beta-\tfrac{1}{2})\ln(n-1+\beta)-(n-1+\beta)+O(k\ln k)\\
=& 
\sum_u\left(
-(\#u-\alpha-\tfrac{1}{2})\ln \#u\right)+(n+\beta-\tfrac{1}{2})\ln
n+O(k\ln k)\\
=& 
-n\sum_u\left(\frac{\#u}{n}\ln\frac{\#u}{n}\right)+(k(\alpha+\tfrac{1}{2})+\beta-\tfrac{1}{2})\ln
  n+O(k\ln k).
\end{split}
\]
So the regret has a factor of 
$(\tfrac{1}{2}+\alpha)k+\beta-\tfrac{1}{2}$ in front of the $\ln n$.
The theoretically optimal rate of $(k-1)/2$ is recovered in the limit
for $\alpha\to0$ and $\beta\to0$, but the latter blows up the relevant
constants. We can set $\alpha=0$, which is analysed below. 

\section{The case $\alpha=0$}
Optimizing $\alpha$ appears to be complicated; in practice on the
natural language example any intermediate value for $\alpha$ performed
better than $\alpha=0$. However, for applications with smaller
effective alphabets, $\alpha=0$ yields the optimal asymptotics.

In this case, we get
\[
\frac{P(x^n)}{P_0(\tn{unique subsequence})}=\frac{\left(\prod_{u\in\U}
    \Gamma(\#u)\right)\cdot \beta^k\Gamma(\beta)}{\Gamma(n+\beta)}.
\]
Somewhat more careful application of Stirling yields
\[
\begin{split}
-\ln\frac{P}{P_0}=&\sum_u\left(
-(\#u-\tfrac{1}{2})\ln(\#u-1)+(\#u-1)-\tfrac{1}{2}\ln(2\pi)\right)-k\ln\beta\\
&+(n+\beta-\tfrac{1}{2})\ln(n+\beta-1)-(n+\beta-1)+O(1)\\
=&\sum_u\left(
-(\#u-\tfrac{1}{2})\ln(\#u-1)\right)-k\left(1+\tfrac{1}{2}\ln(2\pi)+\ln\beta\right)+(n+\beta-\tfrac{1}{2})\ln(n+\beta-1)+O(1)\\
=&\sum_u\left(-\frac{\#u}{n}\ln\frac{\#u}{n}\right)+\left(\frac{k}{2}+\beta-\tfrac{1}{2}\right)\ln n-k\left(1+\tfrac{1}{2}\ln(2\pi)+\ln\beta\right)+O(1).
\end{split}
\]
To optimize $\beta$, we set the derivative to zero, yielding $\ln n =
k/\beta$ or $\beta=k/\ln n$. The relation between
$k$ and $n$ is not known a priori, but a value of $1/2$ or $1$ provides a reasonable bound.

\bibliography{paper}

\end{document}

% look at Joel Veness's work
